---
output: hugodown::md_document
# Documentation: https://sourcethemes.com/academic/docs/managing-content/

title: "How to Use Census Microdata to Analyze High Speed Internet in Kentucky"
subtitle: ""
summary: ""
authors: [admin]
tags: []
categories: []
date: 2020-08-29
lastmod: 2020-08-29
featured: false
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ""
  focal_point: ""
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
---

# Getting the Data

The easiest way to get census microdata is through the Integrated Public Use Microdata Series (IPUMS) hosted by the University of Minnesota. While you can get the data directly from the Census Bureau, IPUMS has made it much easier to compare across multiple years and to select the variables you want. IPUMS also provides a codebook that is easy to refer to and notes any important changes from year to year. 

I've put the data for just Kentucky up on GitHub, so I'll read it in from there.

```{r, message = FALSE, warning = FALSE}
library(tidyverse) 
library(survey)

df <- read_csv("https://raw.github.com/natekratzer/raw_data/master/ky_high_speed_internet.csv")
```

# Cleaning the Data

When downloading the data it's all numeric, even for variables that are categorial - they've been coded and our first step in the analysis will be using the code book to translate them. I won't show all the codebooks, but for this first variable let's take a look at what IPUMS has to say. For all years NA is coded as 00 and No high speed internet is coded as 20. Prior to 2016 there are detailed codes for the type of internet access, while for 2016 and after the code is collapsed. 

![](high_speed_code.png)

I'll use a `case_when()` statement to recode high speed interent access into a categorical variable.

```{r}
# High Speed Internet
df <- df %>%
  mutate(
    hspd_int = case_when(
      CIHISPEED == 00 ~ NA_character_,
      CIHISPEED == 20 ~ "No",
      CIHISPEED >= 10 & CIHISPEED < 20 ~ "Yes",
      TRUE ~ NA_character_
    )
  )
```

## Getting wrong answers by not knowing the data

Now that we have a high speed internet category we can group the data and count up how many responses are in each group. I'll also pivot the dataframe to make it easy to calculate percent with high speed internet. 

```{r}
# Count numbers with and without high speed internet
df_group <- df %>%
  group_by(hspd_int, YEAR) %>%
  summarize(count = n(), .groups = "drop")

# Pivot for easier percent calculations
df_wide <- df_group  %>%
  pivot_wider(id_cols = YEAR, names_from = hspd_int, values_from = count) %>%
  mutate(percent_hspd = (Yes / (Yes + No)),
         percent_NA = (`NA` / (Yes + No + `NA`))) 
```

```{r}
# Count numbers with and without high speed internet
df_group <- df %>%
  group_by(hspd_int, YEAR) %>%
  summarize(count = sum(PERWT))

# Pivot for easier percent calculations
df_wide <- df_group  %>%
  pivot_wider(id_cols = YEAR, names_from = hspd_int, values_from = count) %>%
  mutate(percent_hspd = (Yes / (Yes + No)),
         percent_NA = (`NA` / (Yes + No + `NA`)))

```

This is better. The second problem is harder to spot. There are 3 hints in the data:

1. There is a very high percentage of NA responses. There are more NA answers than there are people who say they don't have high speed access.
2. Percent of of people with high speed access is going down over time, while the number of NA answers is going up.
3. These numbers look very high for Kentucky. 

A sensible guess is that people who say they don't have internet access at all aren't then asked about high speed internet and show up as an NA value when we want to code them as not having high speed interent.

So let's get to know the data a bit better by adding in internet access. We'll do the same analysis, but I'll add internet as another id variable just like year. We can see right away that the answers we have above are only including cases where individuals have internet. 

```{r}
df <- df %>%
  mutate(
    int = case_when(
      CINETHH == 0 ~ NA_character_,
      CINETHH == 1 | CINETHH == 2 ~ "Yes",
      CINETHH == 3 ~ "No",
      TRUE ~ NA_character_
    )
  )

df_group <- df %>%
  group_by(hspd_int, int, YEAR) %>%
  summarize(count = sum(PERWT), .groups = "drop")

# Pivot for easier percent calculations
df_wide <- df_group  %>%
  pivot_wider(id_cols = c(YEAR, int), names_from = hspd_int, values_from = count) %>%
  mutate(percent_hspd = (Yes / (Yes + No)),
         percent_na = (`NA` / (Yes + No + `NA`)))
```

So what we were looking at was the percentage of people with internet who have high speed internet. What we want is the percentage of all people who have high speed internet. We can fix the way we create our categories by saying that anyone who has no internet also has no high speed internet. 

```{r}
df <- df %>%
  mutate(
    int = case_when(
      CINETHH == 0 ~ NA_character_,
      CINETHH == 1 | CINETHH == 2 ~ "Yes",
      CINETHH == 3 ~ "No",
      TRUE ~ NA_character_
    ),
    hspd_int = case_when(
      CIHISPEED == 00 & int != "No" ~ NA_character_,
      CIHISPEED == 20 | int == "No" ~ "No",
      CIHISPEED >= 10 & CIHISPEED < 20 ~ "Yes",
      TRUE ~ NA_character_
    )
  )

# Count numbers with and without high speed internet
df_group <- df %>%
  group_by(hspd_int, YEAR) %>%
  summarize(count = sum(PERWT), .groups = "drop")

# Pivot for easier percent calculations
df_wide <- df_group  %>%
  pivot_wider(id_cols = c(YEAR), names_from = hspd_int, values_from = count) %>%
  mutate(percent_hspd = (Yes / (Yes + No)),
         percent_na = (`NA` / (Yes + No + `NA`)))

```

These results look much better, although still quite a few NA results. 

## Group Quarters in the Census

The census data includes individuals living in group quarters (mostly prisons, senior living centers, and dorms, but includes any sort of communal living arrangement). However, all census questions about appliances and utilities (the category that internet access falls under) are NA for group quarters. So we'll add one more line to filter out individuals living in group quarters (a common practice when working with microdata). The code below adds a filter for Group Quarters. Since this table is showing correct results I'll also add a little additional formatting to make it stand out from the others.

I'll also note that the way the Census Bureau constructs weights is very convenient for getting totals. While I'm focusing on the percent of people who have internet access, the Yes and No columns are accurate estimates of the population with and without access. 

```{r}
# Count numbers with and without high speed internet
df_group <- df %>%
  filter(GQ == 1 | GQ ==2 | GQ == 5) %>%
  group_by(hspd_int, YEAR) %>%
  summarize(count = sum(PERWT), .groups = "drop")

# Pivot for easier percent calculations
df_wide <- df_group  %>%
  pivot_wider(id_cols = c(YEAR), names_from = hspd_int, values_from = count) %>%
  mutate(percent_hspd = (Yes / (Yes + No)),
         percent_na = (`NA` / (Yes + No + `NA`)))

```

That removed about half of our NA values. It might be nice to know a bit more about the missing data, but at around 3 percent of observations it's unlikely to change our substantive conclusions. I suspect these are cases where there wasn't an answer for that question. We'll keep an eye on NA values as we do the analysis, because as we get into questions like how internet access varies by race, income, age, and education we'll want to know if NA answers are more or less likely in any of those categories. 

## Checking against data.census.gov

To do a quick check against the way the census bureau itself analyzes the data I looked at data.census.gov for 2018 in Kentucky. An important note is that their data is for households, and so their numeric counts look quite different because I'm counting number of people. They also have a breakdown where cellular is included in broadband, which I do not want, as a cell phone is not really an adequate work or study device. So to get to what I have we need to add "Broadband such as cable, fiber optic or DSL" and "Satellite Internet service", which gets us to 70.8% compared to the 70.5% in this analysis. The difference is small and most likely the result of their analysis being weighted to the household level rather than the person level. (Internet is measured at the household level and the same for every person in the household, but by choosing to weight it at the person level I am a) letting us talk in terms of people, b) giving more weight to larger households, c) making it possible to breakdown internet access by categories that do vary by household, like age). 

![](data_census_gov.png)

# Analysis

Going forward we're going to want to filter by group quarters, so let's apply that filter to our main dataframe.
```{r}
df <- df %>%
    filter(GQ == 1 | GQ ==2 | GQ == 5) 
```

## Standard Errors

Know that we know the data we'd also like to know how uncertain our sample is so that we know if movements over time are real or just a result of noisy data. There are a few ways to do this. The `survey` package does an excellent job with complex survey designs, but does require learning a new syntax to use. The alternative I'll use here is a method known as bootstrap. IPUMS suggests using bootstrap might be the best way to get standard errors on census microdata. The basic idea of the bootstrap is to resample the existing data and use the sampling error from that as an estimate for sampling error in the overall population. Let's do an example with high speed internet in 2018 to see how it works. The output here will be the mean and standard deviation for Kentucky. (We'll use the standard error to calculate confidence intervals once we start displaying actual results.)

```{r, echo = FALSE}
#set seed
set.seed(42)

# Filter to just 2018
# Exclude NA values
# Recode as numeric vector of 1 and 0
# The numeric 1 and 0 form will make it much easier to get means without pivoting, which matters a lot when doing this 1000 times
df2018 <- df %>%
  filter(YEAR == 2018 & !is.na(hspd_int)) %>%
  mutate(hspd_num = if_else(hspd_int == "Yes", 1, 0)) %>%
  select(hspd_num, PERWT)

# Write a function so I can map over it.
# In this case, we need the function to do the same thing X number of times and assign an ID that we can use as a grouping variable
create_samples <- function(sample_id){
  df_out <- df2018[sample(nrow(df2018), nrow(df2018), replace = TRUE) , ] %>%
    as_tibble()
  df_out$sample_id <- sample_id
  return(df_out)
}

nlist <- as.list(seq(1, 5000, by = 1))
samples <- purrr::map_df(nlist, create_samples)

sample_summary <- samples %>%
  group_by(sample_id) %>%
  mutate(ind_weight = PERWT / sum(PERWT),
         hspd_weight = hspd_num * ind_weight) %>% # PERWT is population and doesn't sum to 1. Rescale it to sum to one
  summarize(group_mean = sum(hspd_weight),
            weight_check = sum(ind_weight), .groups = "drop") # Check that my weights add up to one

display_tbl <- tibble(
  mean = mean(sample_summary$group_mean),
  sd = sd(sample_summary$group_mean)
) 

```

We can also take a look at our bootstrap graphically. We want to check that the distribution of the sample is roughly normal. If it's not, that means we didn't do enough bootstrap samples for the Central Limit Theorem to kick in. 

```{r}
#Check that the distribution is normal and than the middle of the distribution is close to the 70.5% we estimated had internet access above
plt <- ggplot(sample_summary, aes(group_mean)) +
  geom_density() + theme_bw() +
  labs(title = "Bootstrapped means of High Speed Internet Access",
       x = "Mean", 
       y = "Kernel Density")

plt
```

## Checking our results against the survey package

Above we found a mean of 0.705 for 2018 and and standard error of 0.0029 based on our bootstrap analysis. It's worth checking that this is the same result we'd get using an analytic approach (instead of bootstrap).
```{r, echo = FALSE}
library(survey)

# Here we're assuming a simple design. 
# Survey requires the creation of a design object and then has functions that work with that object.
# You can get more complicated, which is when the survey package would be most useful.
svy_df <- svydesign(ids = ~ 1, weights = ~PERWT, data = df2018)

# Taking the mean and standard error from our design object
hint_tbl <- svymean(~hspd_num, design = svy_df)

hint_tbl <- as_tibble(hint_tbl)
names(hint_tbl) <- c("mean", "sd") #The names weren't coerced correctly when transforming into a tibble. 
```

These results are very similar. Following the IPUMS recommendation we'll continue on with the bootstrap, but it's good to know the results are the same for practical purposes. So now instead of just doing 2018, we'll need to do every year. We've already one the mean values for every year, and they're still saved in the `df_wide` variable right now. So let's write a function for bootstrap that will let us find standard errors for every year or for any other grouping we choose. 


## Writing a bootstrap function
```{r, echo = FALSE}
# Create a helper function
# It needs to have a way to recieve the dataframe from the function that calls it, so we've added a second argument
create_samples <- function(sample_id, df){
  
  df_out <- df[sample(nrow(df), nrow(df), replace = TRUE) , ] %>%
    as_tibble()
  
  df_out$sample_id <- sample_id
  
  return(df_out)
}

#Need to be able to take in grouping variables so that the summaries can be specific to the groups
bootstrap_pums <- function(df, num_samples, group_vars) {
  
  nlist <- as.list(seq(1, num_samples, by = 1))
  samples <- purrr::map_df(nlist, create_samples, df)
  
  sample_summary <- samples %>%
    group_by( sample_id, across( {{group_vars}} )) %>%
    mutate(ind_weight = PERWT / sum(PERWT),
           hspd_weight = hspd_n * ind_weight) %>% # PERWT sums to population instead of to 1. Rescale it to sum to 1.
    summarize(group_mean = sum(hspd_weight), .groups = "drop") # Not dropping .groups here results in problems in the next group_by call.
  
  sample_sd <- sample_summary %>%
    group_by( across( {{ group_vars }} )) %>%
    summarize(sd = sd(group_mean), .groups = "drop")
}

# We do need to prep the data a little so that we're not carrying through the whole dataframe.
df_in <- df %>%
   filter(!is.na(hspd_int)) %>%
   mutate(hspd_n = if_else(hspd_int == "Yes", 1, 0)) %>%
   select(hspd_n, PERWT, YEAR)

# And finally we can call the function
boot_results <- bootstrap_pums(df = df_in, num_samples = 100, group_vars = YEAR)
```

Now that we have our bootstrap standard errors we can combine them with the data and plot them. We'll use 95% confidence intervals, which we get by multiplying the standard error by 1.96 (the number of standard deviations that corresponds to a 95% confidence interval).

```{r}
df_plt <- df_wide %>%
  full_join(boot_results, by = "YEAR") %>%
  transmute(Year = YEAR,
            Percent = 100 * percent_hspd,
            me = 100 * 1.96 * sd)
  
plt_int <- ggplot(df_plt, aes(x = Year, y = Percent)) +
  geom_errorbar(aes(ymin = Percent - me, ymax = Percent + me), width = .1) +
  geom_line() +
  geom_point() +
  theme_bw() +
  labs(title = "High Speed Internet Access") +
  theme(legend.position = "bottom")

plt_int
```


## Race, Poverty, Age

### Race

We'll build a table by race and year. It's a long table, so I've added some color to both the `Percent Yes` column and the `Percent NA` column. For the NA column I'm using red to pick out cases where the NA values were particularly high, because we want to see if there's a pattern there. For the Percent Yes column I'm checking to see where the values are particularly low.

```{r}
# Let's build a table first and then we'll do the standard errors

# Coding a race variable using case_when
df <- df %>%
  mutate(race = case_when(
            RACE == 1 ~ "White",
            RACE == 2 ~ "Black",
            RACE > 3 & RACE < 7 ~ "Asian",
            HISPAN > 0 & HISPAN < 5 ~ "Hispanic",
            TRUE ~ "All Others"
          ))

df_group <- df %>%
  group_by(hspd_int, race, YEAR) %>%
  summarize(count = sum(PERWT), .groups = "drop")

# Pivot for easier percent calculations
df_wide <- df_group  %>%
  pivot_wider(id_cols = c(race, YEAR), names_from = hspd_int, values_from = count) %>%
  mutate(percent_hspd = (Yes / (Yes + No)),
         percent_na = (`NA` / (Yes + No + `NA`)))


```

While we do see high NA values in some years


Now let's add standard errors and graph the data.


```{r, echo = FALSE}
# At this point I'll introduce a function to plot multiple groups over time, since we'll use this again 

plt_by <- function(df, group_var, title_text = "High Speed Internet Access by Race and Ethnicity") {
  
  plt <- ggplot(data = df, aes(x = Year, y = Percent, group = {{group_var}}, colour = {{group_var}})) +
    geom_errorbar(aes(ymin = Percent - me, ymax = Percent + me), width = .1) +
    geom_point() +
    geom_line() +
    theme_bw() +
    labs(title = title_text, x = "Year", y = "Percent") +
    theme(legend.position = "bottom")

  plt
}
```


```{r}
# We do need to prep the data a little so that we're not carrying through the whole dataframe.
df_in <- df %>%
  filter(!is.na(hspd_int)) %>%
  mutate(hspd_n = if_else(hspd_int == "Yes", 1, 0)) %>%
  select(hspd_n, PERWT, YEAR, race)

# And we can call the bootstrap function
boot_results <- bootstrap_pums(df = df_in, num_samples = 100, group_vars = c(YEAR, race))

df_plt <- df_wide %>%
  full_join(boot_results, by = c("race", "YEAR")) %>%
  transmute(Year = YEAR,
            Race = race,
            Percent = 100 * percent_hspd,
            me = 100 * 1.96 * sd) %>%
  filter(Race != "All Others") # When plotting All Others overlaps White and having five lines makes it quite hard to read. 



plt_race <- plt_by(df_plt, Race)

plt_race
```

## Poverty Status

```{r}
# Coding a race variable using case_when
df <- df %>%
  mutate(poverty = case_when(
            POVERTY <= 100 ~ "In Poverty",
            POVERTY > 100 & POVERTY <= 200 ~ "Near Poverty",
            TRUE ~ "Not in Poverty"
          ))

df_group <- df %>%
  group_by(hspd_int, poverty, YEAR) %>%
  summarize(count = sum(PERWT), .groups = "drop")

# Pivot for easier percent calculations
df_wide <- df_group  %>%
  pivot_wider(id_cols = c(poverty, YEAR), names_from = hspd_int, values_from = count) %>%
  mutate(percent_hspd = (Yes / (Yes + No)),
         percent_na = (`NA` / (Yes + No + `NA`)))

```


## Age

## Geography

Urban v. Suburban v. Rural


# Mapping the Data

## All of Kentucky

## Children 5-18

We'll also take a count of children 5-18 in Kentucky

```{r}
# Internet Overall

```



## Age

## Geography

Urban v. Suburban v. Rural


# Mapping the Data

## All of Kentucky

## Children 5-18

We'll also take a count of children 5-18 in Kentucky



